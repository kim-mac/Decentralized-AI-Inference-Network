# Decentralized AI Inference Network

A production-style prototype of a **decentralized peerâ€‘toâ€‘peer AI inference system** inspired by Gradient, Bittensor, and Gensyn.

This project demonstrates real-world engineering across:

* Distributed systems & consensus
* Edge AI inference with ONNX
* Reputation & incentive simulation
* Observability + metrics pipeline
* Dockerized multiâ€‘node deployment
* Realâ€‘time dashboard visualization

---

# ğŸš€ Demo Overview

**Flow:**

Coordinator â†’ distributes image task â†’ multiple peers run ONNX inference â†’ majority consensus â†’ reputation updates â†’ metrics exposed via HTTP â†’ dashboard visualizes results.

---

# ğŸ§  Architecture

## Components

### 1. Coordinator

* Registers peers via TCP
* Sends inference tasks to all active peers
* Aggregates predictions
* Computes **majorityâ€‘vote consensus**
* Updates **peer reputation scores**
* Persists metrics to `metrics.json`

### 2. Peer Nodes

* Receive image bytes over socket
* Preprocess to MNIST tensor
* Run **ONNX Runtime inference**
* Return predicted digit

### 3. Metrics Server

* Lightweight HTTP server
* Serves system state at:

```
http://localhost:8000/metrics
```

### 4. React Dashboard (optional extension)

* Displays:

  * Tasks completed
  * Consensus history
  * Peer reputation
  * Active peers

---

# âš™ï¸ Tech Stack

* **Python** â€” networking, orchestration, consensus
* **ONNX Runtime** â€” edge inference execution
* **NumPy / Pillow** â€” preprocessing
* **Multithreading + TCP sockets** â€” distributed coordination
* **JSON persistence** â€” metrics & observability
* **Docker Compose** â€” multiâ€‘node deployment
* **React** â€” visualization dashboard

---

# ğŸ“¦ Project Structure

```
mini-gradient/
â”‚
â”œâ”€â”€ coordinator.py
â”œâ”€â”€ peer.py
â”œâ”€â”€ metrics_server.py
â”œâ”€â”€ create_mnist_onnx.py
â”œâ”€â”€ mnist.onnx
â”œâ”€â”€ digit5.png
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ dashboard/ (React UI)
```

---

# ğŸƒ Local Setup

## 1. Install dependencies

```bash
pip install -r requirements.txt
```

## 2. Start coordinator

```bash
python coordinator.py
```

## 3. Start metrics server

```bash
python metrics_server.py
```

## 4. Launch peers

```bash
python peer.py --id peer1 --port 6001
python peer.py --id peer2 --port 6002
python peer.py --id peer3 --port 6003
```

## 5. Run inference task

Inside coordinator CLI:

```
task digit5.png
```

## 6. View metrics

Open:

```
http://localhost:8000/metrics
```

---

# ğŸ³ Docker Deployment

Run full distributed stack:

```bash
docker compose up --build
```

---

# ğŸ“Š Example Metrics Output

```json
{
  "tasks_completed": 3,
  "consensus_history": [5, 3, 7],
  "reputation": {
    "peer1": 2,
    "peer2": 3,
    "peer3": 1
  },
  "active_peers": ["peer1", "peer2", "peer3"]
}
```

---

# ğŸ” Future Improvements

Planned productionâ€‘grade upgrades:

* Byzantine fault tolerance
* Stakeâ€‘weighted consensus
* Cryptographic proof of inference
* Gossipâ€‘based peer discovery
* Async networking (asyncio / Rust)
* GPU worker marketplace
* Token incentives & slashing

---

# ğŸ¯ Why This Project Matters

Centralized AI compute is becoming a bottleneck for open innovation.

Decentralized inference networks enable:

* Permissionless compute markets
* Trustâ€‘minimized AI execution
* Global edge participation

This repository is a **minimal working prototype** of that future.

---

# â­ If you find this useful

Give the repo a star and feel free to contribute!
